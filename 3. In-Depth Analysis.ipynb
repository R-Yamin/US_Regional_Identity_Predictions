{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Depth Analysis of Region Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning by importing modules necessary for machine learning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_south = pd.read_csv('Clean_Data/clean_south.csv')\n",
    "c_midwest = pd.read_csv('Clean_Data/clean_midwest.csv')\n",
    "c_midwest = c_midwest.rename(columns={'Midwestern?':'Midwestern'})\n",
    "ohe = OneHotEncoder()\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "south=c_south[['Unnamed: 0', 'Southern?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solving issue where Southern? column is in string format and not an object.\n",
    "ast.literal_eval(south['Southern?'][0])\n",
    "\n",
    "#turn each item in the southern? series into a list\n",
    "empty = []\n",
    "for item in south['Southern?']:\n",
    "    \n",
    "    py_lst = ast.literal_eval(item)\n",
    "    empty.append(py_lst)\n",
    "\n",
    "#turning the list back into a series\n",
    "list_southern = pd.Series(empty)\n",
    "c_south = c_south.rename(columns={'Southern?':'Southern'})\n",
    "c_south = c_south.assign(Southern=list_southern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Degree of ident.</th>\n",
       "      <th>Alabama</th>\n",
       "      <th>Arizona</th>\n",
       "      <th>Arkansas</th>\n",
       "      <th>Colorado</th>\n",
       "      <th>Delaware</th>\n",
       "      <th>Florida</th>\n",
       "      <th>Georgia</th>\n",
       "      <th>Illinois</th>\n",
       "      <th>Indiana</th>\n",
       "      <th>...</th>\n",
       "      <th>West Virginia</th>\n",
       "      <th>East North Central</th>\n",
       "      <th>East South Central</th>\n",
       "      <th>Middle Atlantic</th>\n",
       "      <th>Mountain</th>\n",
       "      <th>New England</th>\n",
       "      <th>Pacific</th>\n",
       "      <th>South Atlantic</th>\n",
       "      <th>West North Central</th>\n",
       "      <th>West South Central</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not much</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not at all</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A lot</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not at all</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Degree of ident.  Alabama  Arizona  Arkansas  Colorado  Delaware  Florida  \\\n",
       "0             Some        1        0         0         0         0        1   \n",
       "1         Not much        1        0         0         0         0        1   \n",
       "2       Not at all        0        0         0         0         0        0   \n",
       "3            A lot        1        0         0         0         0        0   \n",
       "4       Not at all        1        0         1         0         0        1   \n",
       "\n",
       "   Georgia  Illinois  Indiana  ...  West Virginia  East North Central  \\\n",
       "0        1         0        0  ...              0                   0   \n",
       "1        1         0        0  ...              1                   0   \n",
       "2        0         0        0  ...              0                   0   \n",
       "3        1         0        0  ...              0                   0   \n",
       "4        1         0        0  ...              1                   0   \n",
       "\n",
       "   East South Central  Middle Atlantic  Mountain  New England  Pacific  \\\n",
       "0                   0                0         0            0        0   \n",
       "1                   0                0         0            0        0   \n",
       "2                   0                0         0            0        0   \n",
       "3                   0                0         0            0        0   \n",
       "4                   0                0         0            0        0   \n",
       "\n",
       "   South Atlantic  West North Central  West South Central  \n",
       "0               1                   0                   0  \n",
       "1               0                   0                   1  \n",
       "2               0                   0                   1  \n",
       "3               0                   0                   1  \n",
       "4               1                   0                   0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating columns with binary values for if a state or region was selected for each respondent\n",
    "mlb.fit_transform(c_south['Southern'])\n",
    "mlb.classes_\n",
    "\n",
    "southern_binary = pd.DataFrame(mlb.fit_transform(c_south['Southern']), columns=mlb.classes_)\n",
    "cr_binary = pd.get_dummies(c_south['Census Region'])\n",
    "south_deg_ident=c_south[['Degree of ident.']]\n",
    "svc_df = south_deg_ident.join(southern_binary)\n",
    "svc_df = svc_df.join(cr_binary)\n",
    "\n",
    "#Dataframe for SVC containing region, states voted, and degree of ident\n",
    "svc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating x's and y's to train models on.\n",
    "ys = svc_df['Degree of ident.']\n",
    "xs = svc_df.drop('Degree of ident.', axis=1)\n",
    "\n",
    "#train_test_split for southern dataset\n",
    "xtrain_s, xtest_s, ytrain_s, ytest_s = train_test_split(xs, ys, test_size=0.3, random_state=42)\n",
    "\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [1, 10, 100, 1000],\n",
       "                          'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
       "                          'kernel': ['rbf']},\n",
       "                         {'C': [1, 10, 100, 1000],\n",
       "                          'coef0': array([1.e-03, 1.e-01, 1.e+01, 1.e+03]),\n",
       "                          'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
       "                          'kernel': ['sigmoid']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to do some cross validation with gridsearchcv to ensure hyperparameters are tuned correctly.\n",
    "#likely there would be overfitting without it.\n",
    "#setting param_grid for 'linear', 'rbf', and 'sigmoid' style kernels to see if other kernels work better.\n",
    "param_grid = [{'C':[1, 10, 100, 1000],\n",
    "               'kernel': ['linear']},\n",
    "             {'C':[1, 10, 100, 1000],\n",
    "              'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']},\n",
    "             {'C':[1, 10, 100, 1000],\n",
    "              'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
    "              'coef0': np.logspace(-3,3,4),\n",
    "              'kernel': ['sigmoid']}]\n",
    "svc_cv = GridSearchCV(svc, param_grid, cv=3)\n",
    "\n",
    "svc_cv.fit(xtrain_s, ytrain_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show which parameters are the best for southern dataset.\n",
    "svc_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       A lot       0.42      0.72      0.53       169\n",
      "  Not at all       0.70      0.66      0.68       257\n",
      "    Not much       0.31      0.14      0.19       111\n",
      "        Some       0.24      0.16      0.19       142\n",
      "\n",
      "    accuracy                           0.49       679\n",
      "   macro avg       0.42      0.42      0.40       679\n",
      "weighted avg       0.47      0.49      0.46       679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get classification report for southern dataset\n",
    "cv_pred = svc_cv.predict(xtest_s)\n",
    "\n",
    "print(classification_report(ytest_s, cv_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the support vector machine algorithm for the southern dataset, we are able to get a predictive model for with about 49% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Midwest Dataset SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turn each item in the midwestern? series into a list\n",
    "midwest_st = []\n",
    "for item in c_midwest['Midwestern']:\n",
    "    \n",
    "    py_lst = ast.literal_eval(item)\n",
    "    midwest_st.append(py_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning the list back into a series\n",
    "list_midwest = pd.Series(midwest_st)\n",
    "c_midwest= c_midwest.assign(Midwestern=list_midwest)\n",
    "\n",
    "#breaking down midwestern states votes and census region into binary values.\n",
    "midwestern_binary = pd.DataFrame(mlb.fit_transform(c_midwest['Midwestern']), columns=mlb.classes_)\n",
    "cr_binary_m = pd.get_dummies(c_midwest['Census Region'])\n",
    "midwest_deg_ident=c_midwest[['Degree of ident.']]\n",
    "\n",
    "#creating a single dataframe with degree of ident, census region, and midwestern states for the SVC.\n",
    "svc_df_m = midwest_deg_ident.join(midwestern_binary)\n",
    "svc_df_m = svc_df_m.join(cr_binary_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Degree of ident.</th>\n",
       "      <th>Arkansas</th>\n",
       "      <th>Colorado</th>\n",
       "      <th>Illinois</th>\n",
       "      <th>Indiana</th>\n",
       "      <th>Iowa</th>\n",
       "      <th>Kansas</th>\n",
       "      <th>Kentucky</th>\n",
       "      <th>Michigan</th>\n",
       "      <th>Minnesota</th>\n",
       "      <th>...</th>\n",
       "      <th>Wyoming</th>\n",
       "      <th>East North Central</th>\n",
       "      <th>East South Central</th>\n",
       "      <th>Middle Atlantic</th>\n",
       "      <th>Mountain</th>\n",
       "      <th>New England</th>\n",
       "      <th>Pacific</th>\n",
       "      <th>South Atlantic</th>\n",
       "      <th>West North Central</th>\n",
       "      <th>West South Central</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not much</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not much</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A lot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Degree of ident.  Arkansas  Colorado  Illinois  Indiana  Iowa  Kansas  \\\n",
       "0         Not much         0         1         0        0     0       0   \n",
       "1         Not much         1         0         1        0     1       0   \n",
       "2            A lot         0         0         1        1     0       0   \n",
       "3            A lot         0         0         1        1     1       0   \n",
       "4             Some         0         0         1        1     1       1   \n",
       "\n",
       "   Kentucky  Michigan  Minnesota  ...  Wyoming  East North Central  \\\n",
       "0         0         0          0  ...        0                   0   \n",
       "1         0         1          1  ...        0                   1   \n",
       "2         0         1          1  ...        0                   1   \n",
       "3         0         0          0  ...        0                   1   \n",
       "4         1         1          0  ...        0                   1   \n",
       "\n",
       "   East South Central  Middle Atlantic  Mountain  New England  Pacific  \\\n",
       "0                   0                0         0            0        0   \n",
       "1                   0                0         0            0        0   \n",
       "2                   0                0         0            0        0   \n",
       "3                   0                0         0            0        0   \n",
       "4                   0                0         0            0        0   \n",
       "\n",
       "   South Atlantic  West North Central  West South Central  \n",
       "0               0                   0                   1  \n",
       "1               0                   0                   0  \n",
       "2               0                   0                   0  \n",
       "3               0                   0                   0  \n",
       "4               0                   0                   0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating x's and y's to train models on for svc_df_m.\n",
    "ym = svc_df_m['Degree of ident.']\n",
    "xm = svc_df_m.drop('Degree of ident.', axis=1)\n",
    "\n",
    "#train_test_split for southern dataset\n",
    "xtrain_m, xtest_m, ytrain_m, ytest_m = train_test_split(xm, ym, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [0.1, 1, 10, 100, 1000],\n",
       "                          'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
       "                          'kernel': ['rbf']},\n",
       "                         {'C': [0.1, 1, 10, 100, 1000],\n",
       "                          'coef0': array([1.e-03, 1.e-01, 1.e+01, 1.e+03]),\n",
       "                          'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
       "                          'kernel': ['sigmoid']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gridsearch for optimal parameters using same parameters as above\n",
    "param_grid = [{'C':[0.1, 1, 10, 100, 1000],\n",
    "               'kernel': ['linear']},\n",
    "             {'C':[0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']},\n",
    "             {'C':[0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
    "              'coef0': np.logspace(-3,3,4),\n",
    "              'kernel': ['sigmoid']}]\n",
    "svc_cv = GridSearchCV(svc, param_grid, cv=3)\n",
    "\n",
    "svc_cv.fit(xtrain_m, ytrain_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show which parameters are the best\n",
    "svc_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       A lot       0.52      0.83      0.64       211\n",
      "  Not at all       0.68      0.92      0.78       289\n",
      "    Not much       0.00      0.00      0.00        87\n",
      "        Some       0.00      0.00      0.00       142\n",
      "\n",
      "    accuracy                           0.61       729\n",
      "   macro avg       0.30      0.44      0.36       729\n",
      "weighted avg       0.42      0.61      0.50       729\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Get classification report\n",
    "cv_pred_m = svc_cv.predict(xtest_m)\n",
    "\n",
    "print(classification_report(ytest_m, cv_pred_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it appears that the 'Not much' and 'Some' labels for the midwest dataset do not produce any precision or recall values. This is interesting considering these are more ambiguious states of self-identification rather than those that really identify as being a southerner (with the \"A lot\" label) and those that do not (with the \"Not at all\" label). It is then with this model of Support Vector Classifier that we are able to get a 61% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model\n",
    "#### Running random forest models for the south and midwestern dataset to see if these yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### South Dataset: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Degree of ident.', 'Alabama', 'Arizona', 'Arkansas', 'Colorado',\n",
       "       'Delaware', 'Florida', 'Georgia', 'Illinois', 'Indiana', 'Kansas',\n",
       "       'Kentucky', 'Louisiana', 'Maryland', 'Mississippi', 'Missouri',\n",
       "       'New Mexico', 'North Carolina', 'Ohio', 'Oklahoma', 'Pennsylvania',\n",
       "       'South Carolina', 'Tennessee', 'Texas', 'Virginia', 'West Virginia',\n",
       "       'East North Central', 'East South Central', 'Middle Atlantic',\n",
       "       'Mountain', 'New England', 'Pacific', 'South Atlantic',\n",
       "       'West North Central', 'West South Central', 'Female', 'Male', '18-29',\n",
       "       '30-44', '45-60', '> 60', '$0 - $24,999', '$100,000 - $149,999',\n",
       "       '$150,000+', '$25,000 - $49,999', '$50,000 - $99,999',\n",
       "       'Associate or bachelor degree', 'Graduate degree', 'High school degree',\n",
       "       'Less than high school degree', 'Some college'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataframe for the random forest classifier for the southern data, spliiting all categorical data into\n",
    "#binary values and columns.\n",
    "rf_df_s = svc_df.join(pd.get_dummies(c_south['Gender']))\n",
    "rf_df_s = rf_df_s.join(pd.get_dummies(c_south['Age Range']))\n",
    "rf_df_s = rf_df_s.join(pd.get_dummies(c_south['Income']))\n",
    "rf_df_s = rf_df_s.join(pd.get_dummies(c_south['Education']))\n",
    "rf_df_s.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting labeled data and features.\n",
    "ys_rf = rf_df_s['Degree of ident.']\n",
    "xs_rf = rf_df_s.drop('Degree of ident.', axis=1)\n",
    "\n",
    "#split training and testing data for random forest classifier\n",
    "xtrain_rf_s, xtest_rf_s, ytrain_rf_s, ytest_rf_s = train_test_split(xs_rf, ys_rf, test_size=0.3, random_state=42)\n",
    "\n",
    "#calling model\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rfc.fit(xtrain_rf_s, ytrain_rf_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:0.9968394437420987\n",
      "Test score:0.4742268041237113\n"
     ]
    }
   ],
   "source": [
    "trainscore = rfc.score(xtrain_rf_s, ytrain_rf_s)\n",
    "testscore = rfc.score(xtest_rf_s, ytest_rf_s)\n",
    "print(f'Training score:{trainscore}')\n",
    "print(f'Test score:{testscore}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_score...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will try to tune hyperparameters to see if can get better test score accuracy\n",
    "max_depth =[int(x) for x in np.linspace(10,110, num=11)]\n",
    "max_depth.append(None)\n",
    "random_grid = {'n_estimators' : [int(x) for x in np.linspace(200, 2000, num=10)],\n",
    "               'max_features' : ['auto','sqrt'],\n",
    "               'max_depth' : max_depth,\n",
    "               'min_samples_split' : [2,5,10],\n",
    "               'min_samples_leaf' : [1,2,4],\n",
    "               'bootstrap':[True, False]}\n",
    "\n",
    "random_search = RandomizedSearchCV(rfc, random_grid, cv=3)\n",
    "\n",
    "#train random search\n",
    "random_search.fit(xtrain_rf_s, ytrain_rf_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 30,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   43.1s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [20, 30, 40, 50],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [8, 10, 12],\n",
       "                         'n_estimators': [700, 800, 900, 1000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_grid = {\n",
    "                      'n_estimators' : [700,800,900,1000],\n",
    "                      'max_depth' : [20,30,40,50],\n",
    "                      'min_samples_split' : [8,10,12],\n",
    "                      'min_samples_leaf' : [1,2,3],\n",
    "                      'bootstrap':[True]}\n",
    "\n",
    "rfc_cv = GridSearchCV(rfc, random_forest_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "rfc_cv.fit(xtrain_rf_s, ytrain_rf_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 50,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 700}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       A lot       0.45      0.59      0.51       169\n",
      "  Not at all       0.64      0.75      0.69       257\n",
      "    Not much       0.29      0.10      0.15       111\n",
      "        Some       0.32      0.27      0.29       142\n",
      "\n",
      "    accuracy                           0.51       679\n",
      "   macro avg       0.43      0.43      0.41       679\n",
      "weighted avg       0.47      0.51      0.48       679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_rf_pred= rfc_cv.predict(xtest_rf_s)\n",
    "\n",
    "print(classification_report(ytest_rf_s, cv_rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the classification report that the accuracy for the south dataset increased to 51% from our max with the support vector machine of 49%. Considering that we were still able to get so close to the 51% accuracy with fewer features goes to show that many of the features from this dataset are not as powerful as the actual census region people are from and which states they voted for as southern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Midwest Dataset: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Degree of ident.', 'Arkansas', 'Colorado', 'Illinois', 'Indiana',\n",
       "       'Iowa', 'Kansas', 'Kentucky', 'Michigan', 'Minnesota', 'Missouri',\n",
       "       'Montana', 'Nebraska', 'North Dakota', 'Ohio', 'Oklahoma',\n",
       "       'Pennsylvania', 'South Dakota', 'West Virginia', 'Wisconsin', 'Wyoming',\n",
       "       'East North Central', 'East South Central', 'Middle Atlantic',\n",
       "       'Mountain', 'New England', 'Pacific', 'South Atlantic',\n",
       "       'West North Central', 'West South Central', 'Female', 'Male', '18-29',\n",
       "       '30-44', '45-60', '> 60', '$0 - $24,999', '$100,000 - $149,999',\n",
       "       '$150,000+', '$25,000 - $49,999', '$50,000 - $99,999',\n",
       "       'Associate or bachelor degree', 'Graduate degree', 'High school degree',\n",
       "       'Less than high school degree', 'Some college'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataframe for the random forest classifier for the midwest data.\n",
    "rf_df_m = svc_df_m.join(pd.get_dummies(c_midwest['Gender']))\n",
    "rf_df_m = rf_df_m.join(pd.get_dummies(c_midwest['Age Range']))\n",
    "rf_df_m = rf_df_m.join(pd.get_dummies(c_midwest['Income']))\n",
    "rf_df_m = rf_df_m.join(pd.get_dummies(c_midwest['Education']))\n",
    "rf_df_m.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting labeled data and features.\n",
    "ym_rf = rf_df_m['Degree of ident.']\n",
    "xm_rf = rf_df_m.drop('Degree of ident.', axis=1)\n",
    "\n",
    "#split training and testing data for random forest classifier\n",
    "xtrain_rf_m, xtest_rf_m, ytrain_rf_m, ytest_rf_m = train_test_split(xm_rf, ym_rf, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_score...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting random search grid to find approximate best parameters for random forest model\n",
    "\n",
    "random_search.fit(xtrain_rf_m, ytrain_rf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 10,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   54.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 648 out of 648 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True, False], 'max_depth': [10, 20, 30],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [8, 10, 12],\n",
       "                         'n_estimators': [900, 1000, 1100, 1200]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_grid = {\n",
    "                      'n_estimators' : [900,1000,1100,1200],\n",
    "                      'max_depth' : [10,20,30],\n",
    "                      'min_samples_split' : [8,10,12],\n",
    "                      'min_samples_leaf' : [1,2,3],\n",
    "                      'bootstrap':[True,False]}\n",
    "\n",
    "rfc_cv = GridSearchCV(rfc, random_forest_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "rfc_cv.fit(xtrain_rf_m, ytrain_rf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 10,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 12,\n",
       " 'n_estimators': 1200}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       A lot       0.55      0.82      0.65       211\n",
      "  Not at all       0.67      0.94      0.78       289\n",
      "    Not much       0.00      0.00      0.00        87\n",
      "        Some       0.14      0.01      0.01       142\n",
      "\n",
      "    accuracy                           0.61       729\n",
      "   macro avg       0.34      0.44      0.36       729\n",
      "weighted avg       0.45      0.61      0.50       729\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "cv_rf_pred=rfc_cv.predict(xtest_rf_m)\n",
    "\n",
    "print(classification_report(ytest_rf_m, cv_rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we again come across the issue of the label of 'not much' yeilding a 0 in precision and recall. While the accruacy still remains close to the 61% that we had reached with the Support Vector Classifier. Yet, we can see that we were able to achieve a f1-score for the those that identified as 'Some' as being considered a midwesterner. This does not mean that it is a much better predictor than our support vector machine, but it is able to measure some qualities for the \"Some\" label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
